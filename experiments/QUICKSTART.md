# Quick Start Guide

## üöÄ Run Your First Experiment

### Option 1: Single Experiment (Fast Test)

```bash
# Run just the baseline (5-10 minutes on GPU)
cd BinaryNet.pytorch
python experiments/mnist_mcmc_experiment.py --loss-type ce --epochs 10
```

### Option 2: Compare All Three (Recommended)

```bash
# Run all experiments with 100 epochs each (~1-2 hours total on GPU)
cd BinaryNet.pytorch
python experiments/run_all_experiments.py
```

Or manually:

```bash
# 1. Baseline: Cross-Entropy
python experiments/mnist_mcmc_experiment.py --loss-type ce --epochs 100

# 2. Vlog (no annealing)
python experiments/mnist_mcmc_experiment.py --loss-type vlog_fixed --beta-fixed 1.0 --epochs 100

# 3. Vlog with Œ≤-annealing (your approach)
python experiments/mnist_mcmc_experiment.py --loss-type vlog_annealing --beta-start 0.1 --beta-end 100.0 --epochs 100
```

## üìä View Results

```bash
# Compare all results
python experiments/compare_results.py

# Or view raw results
cat experiments/results/mnist_*_results.txt
```

## üéØ Expected Output

```
==============================================================================
MNIST BINARY NETWORK - RESULTS COMPARISON
==============================================================================

Loss Function             Final Acc    Best Acc     Best Epoch  
--------------------------------------------------------------------------------
Cross-Entropy             95.00        96.00        85          
Vlog (fixed Œ≤)            94.50        95.80        90          
Vlog (Œ≤-annealing)        96.20        97.10        88          

üèÜ Best Performance: Vlog (Œ≤-annealing) with 97.10% accuracy
   Improvement over Cross-Entropy: +1.10%

==============================================================================
```

## ‚öôÔ∏è Hyperparameter Tuning

If you want to experiment with different parameters:

```bash
# Try different b (tau) values
python experiments/mnist_mcmc_experiment.py --loss-type vlog_annealing --b-value 5.0 --epochs 100
python experiments/mnist_mcmc_experiment.py --loss-type vlog_annealing --b-value 20.0 --epochs 100

# Try different beta ranges
python experiments/mnist_mcmc_experiment.py --loss-type vlog_annealing --beta-start 0.01 --beta-end 50.0 --epochs 100
python experiments/mnist_mcmc_experiment.py --loss-type vlog_annealing --beta-start 1.0 --beta-end 1000.0 --epochs 100

# Try different normalization dimensions
python experiments/mnist_mcmc_experiment.py --loss-type vlog_annealing --normalization-dim 100 --epochs 100
```

## üêõ Troubleshooting

**Error: "No module named 'models.binarized_modules'"**
- Make sure you're in the `BinaryNet.pytorch` directory
- Or add it to PYTHONPATH: `export PYTHONPATH="${PYTHONPATH}:$(pwd)"`

**CUDA out of memory**
- Reduce batch size: `--batch-size 32`

**Training too slow**
- Reduce epochs for testing: `--epochs 10`
- Increase log interval: `--log-interval 200`

## üìù Notes

- **Œ≤-annealing logic**: Œ≤ increases from small‚Üílarge over epochs
  - Small Œ≤ (early): Smooth loss landscape, exploration
  - Large Œ≤ (late): Sharp focus on minimum, exploitation
  
- **b (œÑ) parameter**: Controls potential sharpness (kept constant during training)
  - Smaller b: Smoother potential
  - Larger b: Sharper potential (more like step function)

- **Normalization dim**: sqrt(N) in stability calculation
  - Default: 10 (output dimension)
  - Could also try: 6144 (hidden layer size)

